# -*- coding: utf-8 -*-
"""Adaboost and Logistic.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17yT-lbQz5qSi-pCJr5TTqbiawd7Amrc7
"""

import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
from scipy.special import expit
import seaborn as sns
import numpy as np
from scipy import stats
import warnings
warnings.filterwarnings('ignore')
sns.set_style("darkgrid", {"axes.facecolor": ".9"})

Churn=pd.read_csv("WA_Fn-UseC_-Telco-Customer-Churn.csv")
CreditCard = pd.read_csv("creditcard.csv")
List=list(CreditCard[CreditCard['Class']==0].sample(260000).index)
CreditCard=CreditCard.drop(List,axis=0)

Adult=pd.read_csv("adult.csv")

Adult['workclass']

df= Adult.copy()

df['workclass']=Adult['workclass'].replace({'?':'Private'})

df



"""<h1>Checking for null values</h1>"""

print(Churn.isnull().sum(),Adult.isnull().sum(),CreditCard.isnull().sum())

Adult['occupation']=Adult['occupation'].replace({'?':Adult['occupation'].mode()[0]})
Adult['native.country']=Adult['native.country'].replace({'?':Adult['native.country'].mode()[0]})
Adult['workclass']=Adult['workclass'].replace({'?':Adult['workclass'].mode()[0]})

"""<h1>Correlation Testing</h1>"""

plt.figure(figsize=(7,5))
sns.heatmap(Churn.corr(),annot=True,vmin=-1,vmax=1)
plt.title("Correlation Testing of Churn Features",fontname="Times New Roman",fontweight="bold",fontsize=20)
plt.savefig("Heatmap of Churn.jpg")

"""<p>SeniorCitizen is Positively correlated with MonthlyChange and tenure is Positively Correlated with MonthlyChange column</p>"""

plt.figure(figsize=(18,10))
sns.heatmap(CreditCard.corr(),vmin=-1,vmax=1,cmap='BuPu_r')
plt.title("Correlation Testing of CreditCard Features",fontname="Times New Roman",fontweight="bold",fontsize=20)
plt.savefig("Heatmap of CreditCard.jpg")

"""* Time is negatively Correlated with V3.
* V6,V7,V20 and V21 is Positiively Correlated with mounts.
* V4 annd V11 is Positively Correlated with Class.
"""

plt.figure(figsize=(7,5))
sns.heatmap(Adult.corr(),vmin=-1,vmax=1,annot=True)
plt.title("Correlation Testing of Adult Features",fontname="Times New Roman",fontweight="bold",fontsize=20)
plt.savefig("Heatmap of Adult.jpg")

"""* age is Postively Correlated with capital.gain.
* capital.gain , capital.loss and hours.per.week is Positively Correlated with education.num.
"""

def ScatterPlot(df,Rows,Cols,Title,figsize=(10,10),font=15,Slicing=False):
    """This Function will help to Create the Subplot of Scatter plot of Stong Correlated Columns"""
    matrix=df.corr()  # In this step i am make the Correlation Matrix . This matrix will contain the strength of correalation among different features whether they strongly correlated or not
    for i in matrix.columns:
        matrix.loc[i,i]=0.0   # BY default Correlation matrix give the highest score to correlation by itself feature , So in this loop i am  assign them 0 score 
    if Slicing==False:    # I only want 6 strongly correlated features so if matrix have more than 6 then it will slice then otherwise it does not have to slice.
        Correlation_Dictionary=dict(matrix.abs().idxmax())  # This is the dictionary of 6 strongest correlated columns
    else:
        Correlation_Dictionary=dict(matrix.abs().idxmax()[:6])
    
    plt.figure(figsize=figsize)
    a=Rows  # Number of Rows of subplot.
    b=Cols # Number of Columns for subplot.
    c=1    # Number of Plots i want
    for x,y in Correlation_Dictionary.items():
        plt.suptitle(f'Scatter Plot for {Title}',fontname="Times New Roman",fontweight="bold",fontsize=20)
        plt.subplot(a,b,c,title=f"{x} VS {y}")
        plt.subplots_adjust(hspace=0.5,wspace=0.5)
        sns.scatterplot(df[x],df[y])
        plt.xlabel(f"{x}",fontname="Times New Roman",fontweight="bold",fontsize=font)
        plt.ylabel(f"{x}",fontname="Times New Roman",fontweight="bold",fontsize=font)
        c = c + 1

ScatterPlot(Churn,1,3,"Churn dataset Features",figsize=(14,5)) # Calling the above function (Scatterplot)
plt.savefig("ScatterPlot of Churn Features.jpg")

"""* As you can see there is no strong correlation between the features. Features are Uniformly distributed."""

ScatterPlot(Adult,3,3,"Adult dataset Features",figsize=(14,10)) # Doing the same Step with adult dataset 
plt.savefig("ScatterPlot of Adult Features.jpg") # Saving the plot

"""* there is still no strong correlation between the Adult features. Features are Only Uniformly distributed."""

ScatterPlot(CreditCard,3,3,"CreditCard dataset Features",figsize=(14,10),Slicing=True) # Again doing the same thing with CreditCard dataset
plt.savefig("ScatterPlot of CreditCard Features.jpg") # Saving the plot

"""* The CreditCard Features Also does not have a strongly relationship with each other.So Every dataset Features are not strongly correlated with each other so this will affect the model accuracy.

<h1>Boxplot for Outliers Detection<h1>
"""

def BoxPlotCreator(x,row,col,Title,figsize=(10,10)):
    """The Boxplot helps to visualize the outliers so
        I am going to create a function which will create a subplots of boxplot
    """
    Columns_list=list(x.select_dtypes(include='number').columns)   # This will separate  the Numeric dtypes columns of dataframe
    plt.figure(figsize=figsize) 
    a = row # Number of Rows of Subplot
    b = col # Number of COlumns of subplot
    c = 1   # Number Of plots
    """ THis loop will create the subplots of box plot
    """
    for i in Columns_list:      
        plt.suptitle(f'Box Plots For Outliers Detection of {Title}',fontname="Times New Roman",fontweight="bold",fontsize=20)
        plt.subplot(a,b,c,title=f"{i}")
        plt.subplots_adjust(hspace=0.3,wspace=0.5)
        sns.boxplot(y=x[i])
        plt.ylabel(f"{i}",fontname="Times New Roman",fontweight="bold",fontsize=15)
        c = c + 1

BoxPlotCreator(Churn,1,3,'Churn',figsize=(10,5))  # Calling the above function
plt.savefig("Boxplot of Churn Featurs.jpg") # Saving the plot

BoxPlotCreator(Adult,3,3,'Adult',figsize=(13,10)) # Visualizing the Ouliers of adult dataset
plt.savefig("Boxplot of Adult Featurs.jpg") # Saving the plot

BoxPlotCreator(CreditCard,7,5,'CreditCard',figsize=(18,20)) # Visualizing the outliers of CreditCard dataset
plt.savefig("Boxplot of CreditCard Featurs.jpg")  # Saving the plot

"""<h1>Removing Outiers</h1>"""

Columns_list=list(Churn.select_dtypes(include='number').columns)
data = Churn[Churn['Churn']=='No']
Clean_data=list(data[(np.abs(stats.zscore(data[Columns_list])) >1).all(axis=1)].index)

from scipy import stats
  Columns_list=list(Churn.select_dtypes(include='number').columns)
  data = Churn[Churn['Churn']=='No']
  Clean_data=list(data[(np.abs(stats.zscore(data[Columns_list])) >2).all(axis=1)].index)
  Churn.drop(Clean_data,axis=0)

def Outlier_detectors(df,Target,Maj_class=0):
  """Function which will detect the outliers
      I am using here the z score methode to remove them form stats .scipy
  """
  from scipy import stats
  Columns_list=list(df.select_dtypes(include='number').columns)
  Data = df[df[Target]==Maj_class]
  Outliers=list(Data[(np.abs(stats.zscore(Data[Columns_list])) >2).all(axis=1)].index)
  return df.drop(Outliers,axis=0)

Clean_Churn=Outlier_detectors(Churn,'Churn',Maj_class='No')
Clean_Adult=Outlier_detectors(Adult,'income',Maj_class=0)
Clean_CreditCard=Outlier_detectors(CreditCard,'Class',Maj_class=0)

"""# Checking for balance of classes"""

sns.countplot(Adult['income']) # Plotting the Counts Adult Target column classes to see wether they are balanced or not.
plt.xlabel("Income",fontname="Times New Roman",fontsize=15,fontweight="bold")
plt.ylabel("Count",fontname="Times New Roman",fontsize=15,fontweight="bold")
plt.title("Classes of Adult Target column",fontname="Times New Roman",fontweight="bold",fontsize=20)
plt.savefig('Countpot of Adult Target column.jpg')

# Plotting the counts C Churn target column Classes to see whether they are balanced or not
sns.countplot(Churn['Churn'])
plt.xlabel("Churn",fontname="Times New Roman",fontsize=15,fontweight="bold")
plt.ylabel("Count",fontname="Times New Roman",fontsize=15,fontweight="bold")
plt.title("Classes of Churn Target column",fontname="Times New Roman",fontweight="bold",fontsize=20)
plt.savefig('Countpot of Churn Target column.jpg')

# Ploting the counts of  Creditcard Target column classe to see whether they are balanced or not.
sns.countplot(CreditCard['Class'])
plt.xlabel("Class",fontname="Times New Roman",fontsize=15,fontweight="bold")
plt.ylabel("Count",fontname="Times New Roman",fontsize=15,fontweight="bold")
plt.title("Classes of CreditCard Target column",fontname="Times New Roman",fontweight="bold",fontsize=20)
plt.savefig('Countpot of CreditCard Target column.jpg')

def DataPreprocessing(df,Target,Over=False,y_change=False,n_components=6):
    """ This function will help to do A lot of Data preprocessing  like Imbalance
    the classes of target column , Data Normalization, Dimension Reduction and data
    spliting into training and testing. There are a 3 datesets so would be hard to preprocesse 
    them separately but this function will help to do preprocessing in single step
    """
    """This function will take 4 parameters dataframe,Target COlumn ,Over, y_change and n_components
    """
    # Parameters Description:
    # df: Dataframe (data points)
    # Over : This is by default is false , If you want to oversample sample Imbalance classe then you have to set it True ,oTherwise it will be consider it as undersampling
    # n_components : Number of components  for Dimension reduction by default it is 6.
    X = df.drop(Target,axis=1)
    y = df[Target]
    if y_change==True:
        y[y==0]=-1
    else:
        y
    if Over==False:
        from imblearn.under_sampling import RandomUnderSampler
        rus = RandomUnderSampler(random_state=0)
        X_Resampled,y_Resampled=rus.fit_resample(X,y)
        from sklearn.preprocessing import StandardScaler
        Scaler=StandardScaler()
        X_Scaled=Scaler.fit_transform(X_Resampled)
        from sklearn.decomposition import PCA
        pca = PCA(n_components=n_components)
        X_decomposed = pca.fit_transform(X_Scaled)
        from sklearn.model_selection import train_test_split
        X_train,X_test,y_train,y_test = train_test_split(X_decomposed,y_Resampled,test_size=0.20,random_state=42)
    else:
        from imblearn.over_sampling import RandomOverSampler
        ros = RandomOverSampler()
        X_Resampled,y_Resampled=ros.fit_resample(X,y)
        from sklearn.preprocessing import StandardScaler
        Scaler=StandardScaler()
        X_Scaled=Scaler.fit_transform(X_Resampled)
        from sklearn.decomposition import PCA
        pca = PCA(n_components=n_components)
        X_decomposed = pca.fit_transform(X_Scaled)
        from sklearn.model_selection import train_test_split
        X_train,X_test,y_train,y_test = train_test_split(X_decomposed,y_Resampled,test_size=0.20,random_state=42)
    return X_train,X_test,y_train,y_test

def Labelizer(df):
  """THis function will Labelize the Categorial features with
      the help of hot labelEncoder
  """
  from sklearn.preprocessing import LabelEncoder
  Columns_list=list(df.select_dtypes(include='object').columns) # Seprarating the object dtype columns for labelling
  le = LabelEncoder()
  for x in df[Columns_list]:
    df[x] = le.fit_transform(df[x])
  return df

Train_Churn = Labelizer(Clean_Churn)
Train_adult = Labelizer(Clean_adult)

"""<h1>AdaBoostClassifier:</h1>
<p>AdaBoost also called Adaptive Boosting is a technique in Machine Learning used as an Ensemble Method. The most common algorithm used with AdaBoost is decision trees with one level that means with Decision trees with only 1 split. These trees are also called Decision Stumps.So I will Creat first DecisionStump and then Build the adaboost Model.</p>

<h2>Statistical Approach:</h2>
"""

class DecisionStump():
    def __init__(self):
        self.polarity=1
        self.feature_idx=None
        self.threshold = None
        self.alpha = None
    def predict(self,X):
        n_samples = X.shape[0]
        X_column = X[:,self.feature_idx]
        predictions = np.ones(n_samples)
        if self.polarity == 1:
            predictions[X_column<self.threshold] =-1
        else:
            predictions[X_column>self.threshold] = 1
        return predictions

class Adaboost():
    def __init__(self,n_clf=5):
        self.n_clf=n_clf
    def fit(self,X,y):
        n_samples,n_features = X.shape
        w = np.full(n_samples,(1/n_samples))
        
        self.clfs = []
        for _ in range(self.n_clf):
            clf =DecisionStump()
            
            min_error = float('inf')
            for feature_i in range(n_features):
                X_column = X[:,feature_i]
                thresholds = np.unique(X_column)
                for threshold in thresholds:
                    p = 1
                    predictions = np.ones(n_samples)
                    predictions[X_column<threshold] =-1
                    misclassified = w[y !=predictions]
                    error = sum(misclassified)
                    if error >0.5:
                        error = 1-error
                        p=-1
                    if error<min_error:
                        min_error = error
                        clf.polarity = p
                        clf.threshold = threshold
                        clf.feature_idx = feature_i
            EPS =1e-10
            clf.alpha = 0.5*np.log((1-error)/(error+EPS))
            predictions = clf.predict(X)
            w+=np.exp(-clf.alpha*y*predictions)
            w/=np.sum(w)
            
            self.clfs.append(clf)
    def predict(self,X):
        clf_preds = [clf.alpha*clf.predict(X) for clf in self.clfs]
        y_pred = np.sum(clf_preds,axis=0)
        y_pred = np.sign(y_pred)
        return y_pred

def  Confusion_matrix(y_actual, y_predicted):
      tp = 0
      tn = 0
      fp = 0
      fn = 0
      for i in range(len(y_actual)):
        if y_actual.iloc[i] > 0:
          if y_actual.iloc[i] == y_predicted[i]:
             tp = tp + 1
          else:
             fn = fn + 1
        if y_actual.iloc[i] < 1:
            if y_actual.iloc[i] == y_predicted[i]:
                 tn = tn + 1
            else:
                fp = fp + 1 
      cm = [[tn, fp], [fn, tp]]
      return tp,tn,fp,fn,cm

def accuracy(y_true,y_pred):
    accuracy = np.sum(y_true==y_pred)/len(y_true)
    return accuracy

def Model_DataFrame(y_test,y_pred,Title):
  Accuracy = accuracy(y_test,y_pred)
  tp,tn,fp,fn,cm=Confusion_matrix(y_test,y_pred)
  f1 = tp /(tp+(1/2)*(fp+fn))
  Recall = tp/(tp+fn)
  try:
    precision = tp / (tp + fp)
  except ZeroDivisionError:
    precision = 0
    return precision
  Frame=pd.DataFrame({"True Positive":tp,"True Negative":tn,"False Positive":fp,"False Negative":fn,"Accuracy":Accuracy,"Recall":Recall,"Precision":precision},index=[0]).T
  return Frame.rename({0:Title},axis=1)

"""# Adaboost with Churn"""

X_train_Churn,X_test_Churn,y_train_Churn,y_test_Churn=DataPreprocessing(Train_Churn,'Churn',Over=True,y_change=True,n_components=7)

Ada_Churn=Adaboost(n_clf=7)
Ada_Churn.fit(X_train_Churn,y_train_Churn)

y_pred_Churn=Ada_Churn.predict(X_test_Churn)
accuracy(y_test_Churn,y_pred_Churn)

"""# Make A dataFrame of Model Results"""

y_test_Churn[y_test_Churn==-1]=0
y_pred_Churn[y_pred_Churn==-1]=0
Chun_ada_df=Model_DataFrame(y_test_Churn,y_pred_Churn,"AdaBoost on Churn")

tp,tn,fp,fn,cm=Confusion_matrix(y_test_Churn,y_pred_Churn)
sns.heatmap(cm,annot=True,fmt='g')

"""# Adaboost with Adult"""

X_train_Adult,X_test_Adult,y_train_Adult,y_test_Adult=DataPreprocessing(Train_adult,'income',Over=False,n_components=4,y_change=True)

Ada_Adult=Adaboost(n_clf=4)
Ada_Adult.fit(X_train_Adult,y_train_Adult)

y_pred_Adult=Ada_Adult.predict(X_test_Adult)
accuracy(y_test_Adult,y_pred_Adult)

X_test_Adult[X_test_Adult==-1]=0
y_pred_Adult[y_pred_Adult==-1]=0
Adult_ada_df=Model_DataFrame(y_test_Adult,y_pred_Adult,"AdaBoost on Adult")
Adult_ada_df

"""# Adaboost with CreditCard"""

X_train_Credit,X_test_Credit,y_train_Credit,y_test_Credit=DataPreprocessing(CreditCard,'Class',n_components=10,y_change=True)

Ada_Credit=Adaboost(n_clf=10)
Ada_Credit.fit(X_train_Credit,y_train_Credit)

y_pred_Credit=Ada_Credit.predict(X_test_Credit)
accuracy(y_test_Credit,y_pred_Credit)

y_test_Credit[y_test_Credit==-1]=0
y_pred_Credit[y_pred_Credit==-1]=0

CreditCard_ada_results=Model_DataFrame(y_test_Credit,y_pred_Credit,"AdaBoost on CreditCard")
CreditCard_ada_results

tp,tn,fp,fn,cm=Confusion_matrix(y_test_Credit,y_pred_Credit)

sns.heatmap(cm,annot=True,fmt='g')

"""<h1>Logistic Regression</h1>"""

class LogisticRegressionTanh:
     def __init__(self,lr=0.001,n_iters=1000):
            self.lr =lr
            self.n_iters = n_iters
            self.weights = None
            self.bias = None

      #Tan hyperbolic Function
     def Tanh(self,x):
        tanh = (np.exp(x)-np.exp(-x))/np.exp(x)+np.exp(-x)
        return tanh

    #Method for calculating the gradients


     def fit(self,x,y):
        m,n = x.shape
        self.weights =np.zeros(n)
        self.bias = 0
        for _ in range(self.n_iters):
            linear_model = np.dot(x,self.weights) + self.bias
            y_pred = self.Tanh(linear_model)
            dw = (1/n)*np.dot(x.T,(y_pred-y))
            db = (1/n)*np.sum(y_pred-y)

            #Updating the weights
            self.weights -= self.lr*dw
            self.bias -= self.lr*db

     #Method to predict the class label.
     def predict(self, x):
        linear_model = np.dot(x,self.weights) + self.bias
        y_pred = self.Tanh(linear_model)
        Y_pred_final = [1 if i >0.5 else 0 for i in y_pred]
        return np.array(Y_pred_final)

"""# Building  LR Model on Churn dataset"""

Train_Churn_LR = Labelizer(Clean_Churn)
Train_adult_LR = Labelizer(Clean_adult)

X_train_Churn_LR , X_test_Churn_LR , y_train_Churn_LR , y_test_Churn_LR = DataPreprocessing(Train_Churn_LR,'Churn',Over=True)

lr_churn =LogisticRegressionTanh(lr=0.01,n_iters=3000)
lr_churn.fit(X_train_Churn_LR,y_train_Churn_LR)

y_pred_Churn=lr_churn.predict(X_test_Churn_LR)
accuracy(y_test_Churn_LR,y_pred_Churn)

Churn_LR_df=Model_DataFrame(y_test_Churn_LR,y_pred_Churn,"Logistic on Churn")
Churn_LR_df

tp,tn,fp,fn,cm=Confusion_matrix(y_test_Churn_LR,y_pred_Churn)
sns.heatmap(cm,annot=True,fmt='g')

"""# Building  LR Model on Adult Dataset"""

X_train_Adult_LR , X_test_Adult_LR , y_train_Adult_LR , y_test_Adult_LR = DataPreprocessing(Train_adult,'income',Over=True)

lr_Adult=LogisticRegressionTanh(lr=0.001,n_iters=3000)
lr_Adult.fit(X_train_Adult_LR,y_train_Adult_LR)

y_pred_Adult=lr_Adult.predict(X_test_Adult_LR)
accuracy(y_test_Adult_LR,y_pred_Adult)

Adult_LR_df=Model_DataFrame(y_test_Adult_LR,y_pred_Adult,"Logistic on Adult")
Adult_LR_df

tp,tn,fp,fn,cm=Confusion_matrix(y_test_Adult_LR,y_pred_Adult)
sns.heatmap(cm,annot=True,fmt='g')

"""# Building LR Model on CredictCard Dataset"""

X_train_Credit_LR,X_test_Credit_LR,y_train_Credit_LR,y_test_Credit_LR = DataPreprocessing(CreditCard,'Class',n_components=11)

lr_Credit=LogisticRegressionTanh(lr=0.001,n_iters=3000)
lr_Credit.fit(X_train_Credit_LR,y_train_Credit_LR)

y_pred_Credit=lr_Credit.predict(X_test_Credit_LR)
accuracy(y_test_Credit_LR,y_pred_Credit)

Credit_LR_df=Model_DataFrame(y_test_Credit_LR,y_pred_Credit,"Logistic on CreditCard")
Credit_LR_df

tp,tn,fp,fn,cm=Confusion_matrix(y_test_Credit_LR,y_pred_Credit)
sns.heatmap(cm,annot=True,fmt='g')

